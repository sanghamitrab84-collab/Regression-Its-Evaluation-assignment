{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is Simple Linear Regression? **\n",
        "\n",
        "  **Answeer:** Simple Linear Regression (SLR) is a statistical method used to study the relationship between two continuous variables ‚Äî one independent variable (predictor) and one dependent variable (response). It helps in predicting the value of the dependent variable based on the known value of the independent variable by fitting a straight line through the data points.\n",
        "\n",
        "  The equation for simple linear regression is:\n",
        "\n",
        "Y=a+bX+e\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable (response)\n",
        "\n",
        "X = Independent variable (predictor)\n",
        "\n",
        "a = Intercept (value of Y when X = 0)\n",
        "\n",
        "b = Slope (change in Y for a unit change in X)\n",
        "\n",
        "e = Error term (difference between actual and predicted value)\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The goal of SLR is to find the best-fitting straight line through the data points such that the sum of squared differences (errors) between the observed and predicted values is minimized.\n",
        "\n",
        "This line is called the regression line or line of best fit.\n",
        "\n",
        "The slope (b) shows the direction and strength of the relationship:\n",
        "\n",
        "If b > 0, the relationship is positive (as X increases, Y increases).\n",
        "\n",
        "If b < 0, the relationship is negative (as X increases, Y decreases).\n",
        "\n",
        "Assumptions of Simple Linear Regression:\n",
        "\n",
        "Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals (errors) is constant across all levels of X.\n",
        "\n",
        "Normality: The residuals are normally distributed.\n",
        "\n",
        "Steps in Performing Simple Linear Regression:\n",
        "\n",
        "Collect Data for the two variables (X and Y).\n",
        "\n",
        "Plot a Scatter Diagram to visualize the relationship.\n",
        "\n",
        "Calculate Regression Coefficients (a and b) using formulas:\n",
        "b=(n(Œ£XY)-(Œ£X)(Œ£Y))/(n(„ÄñŒ£X„Äó^2 )-(Œ£„ÄñX)„Äó^2 )\n",
        "a=(‚àëY-b(‚àëX))/n\n",
        "\n",
        "Form the Regression Equation\n",
        "Y=a+bX.\n",
        "\n",
        "Predict Values of Y for given X.\n",
        "\n",
        "Evaluate Model Accuracy using measures such as R¬≤ (Coefficient of Determination).\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose a company wants to predict sales (Y) based on advertising expenditure (X).\n",
        "If the regression equation is:\n",
        "Y=25+5X\n",
        "\n",
        "This means:\n",
        "\n",
        "When advertising = 0, sales = 25 (intercept).\n",
        "\n",
        "For every 1 unit increase in advertising, sales increase by 5 units (slope).\n",
        "\n",
        "Applications:\n",
        "\n",
        "Predicting demand, sales, or revenue.\n",
        "\n",
        "Analyzing the effect of one variable on another (e.g., temperature vs. electricity consumption).\n",
        "\n",
        "Estimating trends in economics, business, and engineering.\n",
        "\n",
        "**Question 2: What are the key assumptions of Simple Linear Regression? **\n",
        "      **Answer:**Simple Linear Regression (SLR) is based on several important statistical assumptions that must be satisfied to ensure that the model‚Äôs estimates, predictions, and conclusions are reliable and accurate. These assumptions describe how the variables and the errors (residuals) should behave in the regression model.\n",
        "\n",
        "The key assumptions are explained below:\n",
        "\n",
        "1. Linearity\n",
        "\n",
        "The relationship between the independent variable (X) and the dependent variable (Y) should be linear.\n",
        "\n",
        "This means that a change in X leads to a proportional change in Y.\n",
        "\n",
        "The scatter plot of X and Y should show a straight-line pattern.\n",
        "\n",
        "Example: If advertising expenditure increases, sales should increase or decrease in a roughly straight-line manner.\n",
        "\n",
        "2. Independence of Errors (4 Marks)\n",
        "\n",
        "The residuals (errors) should be independent of each other.\n",
        "\n",
        "In other words, the value of one residual should not depend on another.\n",
        "\n",
        "This is especially important when data are collected over time (to avoid autocorrelation).\n",
        "\n",
        "Violation Example: In time series data, if one year‚Äôs error affects the next year‚Äôs error, the assumption is violated.\n",
        "\n",
        "3. Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "The variance of residuals should be constant across all levels of X.\n",
        "\n",
        "This means the spread of residuals should remain the same for small and large values of X.\n",
        "\n",
        "If the spread increases or decreases, it is called heteroscedasticity, which reduces reliability.\n",
        "\n",
        "Example: When plotting residuals vs. predicted values, the points should be evenly spread (no funnel shape).\n",
        "\n",
        "4. Normality of Residuals\n",
        "\n",
        "The residuals (errors) should be normally distributed with a mean of zero.\n",
        "\n",
        "This ensures that statistical tests like t-tests and F-tests used in regression are valid.\n",
        "\n",
        "A histogram or Q-Q plot of residuals can be used to check this assumption.\n",
        "\n",
        "Example: Most residuals should cluster around zero with few large errors.\n",
        "\n",
        "5. No Multicollinearity (for multiple regression; in SLR it means only one X)\n",
        "\n",
        "In Simple Linear Regression, there is only one independent variable, so multicollinearity does not apply.\n",
        "\n",
        "However, in multiple regression, independent variables should not be highly correlated with each other.\n",
        "\n",
        "6. Measurement Accuracy\n",
        "\n",
        "The variables (X and Y) should be measured accurately and without bias.\n",
        "\n",
        "Measurement errors in variables can lead to incorrect estimates of slope and intercept.\n",
        "Meeting these assumptions ensures that the Simple Linear Regression model provides an accurate, unbiased, and reliable understanding of the relationship between variables. Violating these assumptions can lead to incorrect predictions and misleading interpretations.\n",
        "\n",
        "**Question 3: What is heteroscedasticity, and why is it important to address in regression models? **\n",
        "      **Answer: **Heteroscedasticity refers to a situation in regression analysis where the variance of the errors (residuals) is not constant across all levels of the independent variable(s).\n",
        "In simple terms, it means that the spread of residuals increases or decreases as the value of the independent variable changes.\n",
        "\n",
        "When the variance of residuals remains the same, it is called homoscedasticity.\n",
        "When it changes, the data exhibit heteroscedasticity.\n",
        "In a regression model:\n",
        "Y=a+bX+e\n",
        "If the variance of the error term\n",
        " is not constant, i.e.,\n",
        "Var(ei)‚â†œÉ^2\n",
        "then heteroscedasticity is present.\n",
        "Ideally, we should have:\n",
        "Var(ei)=œÉ2(constant)\n",
        "Graphical Illustration :\n",
        "\n",
        "When plotting residuals vs. predicted values:\n",
        "\n",
        "In a homoscedastic case, residuals are evenly scattered around zero.\n",
        "\n",
        "In a heteroscedastic case, residuals show a pattern, such as a funnel shape ‚Äî either widening or narrowing as X increases.\n",
        "\n",
        "Causes of Heteroscedasticity :\n",
        "\n",
        "Large differences in the scale of data (e.g., income vs. expenditure).\n",
        "\n",
        "Outliers or extreme values in the data.\n",
        "\n",
        "Model misspecification (missing key variables).\n",
        "\n",
        "Nonlinear relationships between variables.\n",
        "\n",
        "Cross-sectional data with groups having different variability (e.g., rich vs. poor households).\n",
        "\n",
        "Why It Is Important to Address :\n",
        "\n",
        "Heteroscedasticity violates one of the key assumptions of regression ‚Äî constant variance of errors (homoscedasticity).\n",
        "If it is not addressed, it can lead to several serious problems:\n",
        "\n",
        "Unreliable Hypothesis Tests:\n",
        "\n",
        "Standard errors of coefficients become biased.\n",
        "\n",
        "t-tests and F-tests become invalid, leading to wrong conclusions about significance.\n",
        "\n",
        "Inefficient Estimates:\n",
        "\n",
        "The regression coefficients (a and b) remain unbiased, but they are no longer efficient (do not have the minimum variance).\n",
        "\n",
        "Incorrect Confidence Intervals and Predictions:\n",
        "\n",
        "Confidence intervals may be too wide or too narrow.\n",
        "\n",
        "Predictions become less accurate.\n",
        "\n",
        "Distorted Model Fit:\n",
        "\n",
        "Model performance metrics such as R¬≤ can become misleading.\n",
        "\n",
        "Detection Methods :\n",
        "\n",
        "Graphical: Plot residuals vs. fitted values (look for patterns).\n",
        "\n",
        "Statistical Tests:\n",
        "\n",
        "Breusch‚ÄìPagan test\n",
        "\n",
        "White‚Äôs test\n",
        "\n",
        "Goldfeld‚ÄìQuandt test\n",
        "\n",
        "Remedies for Heteroscedasticity :\n",
        "\n",
        "Transform Variables: Use logarithmic or square root transformation of Y or X.\n",
        "\n",
        "Use Weighted Least Squares (WLS): Assign weights inversely proportional to variance.\n",
        "\n",
        "Use Robust Standard Errors: To correct standard error bias without changing coefficients.\n",
        "\n",
        "Conclusion :\n",
        "\n",
        "Heteroscedasticity is a common issue in regression models that violates the assumption of constant variance of errors.\n",
        "Addressing it is essential for obtaining reliable estimates, valid hypothesis tests, and accurate predictions in statistical modeling.\n",
        "\n",
        "**Question 4: What is Multiple Linear Regression?**\n",
        "\n",
        " ** Answer:** Multiple Linear Regression (MLR) is a statistical technique used to study the relationship between one dependent variable (Y) and two or more independent variables (X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, ‚Ä¶, X‚Çô).\n",
        "It helps to predict the value of the dependent variable based on several predictors and to understand how each independent variable affects the outcome, while keeping others constant.\n",
        "\n",
        "It is an extension of Simple Linear Regression, which involves only one independent variable.\n",
        "\n",
        "Mathematical Model :\n",
        "\n",
        "The general equation for multiple linear regression is:\n",
        "Y=a+b1X1+b2X2+b3X3+....+bnXn+e\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Dependent (response) variable\n",
        "\n",
        "X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çô = Independent (predictor) variables\n",
        "\n",
        "a = Intercept (value of Y when all Xs = 0)\n",
        "\n",
        "b‚ÇÅ, b‚ÇÇ, ‚Ä¶, b‚Çô = Regression coefficients (show how much Y changes for a one-unit change in each X, keeping others constant)\n",
        "\n",
        "e = Error term (difference between observed and predicted Y)\n",
        "\n",
        "Explanation (4 Marks):\n",
        "\n",
        "MLR fits a plane (or hyperplane) instead of a line (as in simple regression).\n",
        "\n",
        "The objective is to find the best-fitting model that minimizes the sum of squared residuals.\n",
        "\n",
        "Each coefficient shows the partial effect of an independent variable on Y while holding other variables fixed.\n",
        "\n",
        "Example:\n",
        "Predicting a student‚Äôs final score (Y) based on:\n",
        "\n",
        "Hours studied (X‚ÇÅ)\n",
        "\n",
        "Attendance percentage (X‚ÇÇ)\n",
        "\n",
        "Internal assessment marks (X‚ÇÉ)\n",
        "\n",
        "Regression model:\n",
        "\n",
        "ùëå=10+2ùëã1+0.5ùëã2+1.2ùëã3\n",
        "\n",
        "This means:\n",
        "\n",
        "For every additional hour studied, score increases by 2 marks (keeping X‚ÇÇ and X‚ÇÉ constant).\n",
        "\n",
        "For every 1% increase in attendance, score increases by 0.5 marks, and so on.\n",
        "\n",
        "Assumptions of Multiple Linear Regression :\n",
        "\n",
        "Linearity: Relationship between dependent and independent variables is linear.\n",
        "\n",
        "Independence of Errors: Residuals are independent of each other.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No Multicollinearity: Independent variables are not highly correlated with each other.\n",
        "\n",
        "Applications :\n",
        "\n",
        "Predicting outcomes in business, economics, and engineering.\n",
        "\n",
        "Forecasting sales, production, or demand.\n",
        "\n",
        "Analyzing the effect of multiple factors on performance or output.\n",
        "\n",
        "Used in machine learning for predictive modeling.\n",
        "\n",
        "Advantages :\n",
        "\n",
        "Allows analysis of multiple factors simultaneously.\n",
        "\n",
        "Provides a more accurate and realistic model than simple regression.\n",
        "\n",
        "Limitations :\n",
        "\n",
        "Requires large datasets and careful checking of assumptions.\n",
        "\n",
        "Sensitive to multicollinearity and outliers.\n",
        "\n",
        "Conclusion :\n",
        "\n",
        "Multiple Linear Regression is a powerful statistical tool used to model and predict the relationship between a dependent variable and several independent variables.\n",
        "It helps in decision-making, forecasting, and understanding the combined influence of multiple factors on a particular outcome.\n",
        "\n",
        "**Question 5**:What is polynomial regression, and how does it differ from linear regression?\n",
        "**Answer:** Definition :\n",
        "\n",
        "Polynomial Regression is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n",
        "It is used when the data show a curved (nonlinear) relationship, which a simple linear regression cannot accurately capture.\n",
        "\n",
        "Polynomial regression is still a linear model in parameters, but nonlinear in the variable X.\n",
        "Mathematical Model :\n",
        "\n",
        "The general form of a polynomial regression equation is:\n",
        "ùëå=ùëé0+ùëé1ùëã+ùëé2ùëã2+ùëé33+‚ãØ+ùëéùëõùëãùëõ+ùëí\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable (response)\n",
        "\n",
        "X = Independent variable (predictor)\n",
        "\n",
        "a‚ÇÄ, a‚ÇÅ, a‚ÇÇ, ‚Ä¶, a‚Çô = Coefficients of the model\n",
        "\n",
        "n = Degree of the polynomial\n",
        "\n",
        "e = Error term (random error)\n",
        "\n",
        "Example:\n",
        "If the relationship between X and Y is curved, a quadratic (second-degree) model can be used:\n",
        "Y=a0+a1X+a2X2+e\n",
        "Explanation :\n",
        "\n",
        "Linear regression assumes a straight-line relationship between X and Y.\n",
        "\n",
        "Polynomial regression allows the line to curve to fit more complex relationships.\n",
        "\n",
        "It can model U-shaped, inverted U-shaped, or other nonlinear patterns.\n",
        "\n",
        "Polynomial regression is often applied when residual plots from a linear regression show a systematic curved pattern rather than random scatter.\n",
        "\n",
        "Example :\n",
        "\n",
        "Suppose you are studying the relationship between temperature (X) and engine efficiency (Y).\n",
        "The relationship is not straight ‚Äî efficiency increases up to a point and then decreases.\n",
        "A quadratic regression model might fit this pattern:\n",
        "Y=30+5X‚àí0.2X2\n",
        "\n",
        "Here, the negative coefficient of\n",
        "X2\n",
        " causes the curve to bend downwards.\n",
        "\n",
        "Advantages :\n",
        "\n",
        "Captures nonlinear relationships between variables.\n",
        "\n",
        "Provides better fitting models for curved data.\n",
        "\n",
        "Limitations :\n",
        "\n",
        "May lead to overfitting if a high-degree polynomial is used.\n",
        "\n",
        "Interpretation of coefficients becomes difficult.\n",
        "\n",
        "Extrapolation beyond data range can be highly inaccurate.\n",
        "\n",
        "Conclusion :\n",
        "\n",
        "Polynomial Regression is a powerful extension of linear regression that can model curved relationships by including higher-degree terms of the independent variable.\n",
        "While it improves flexibility and accuracy for nonlinear data, it should be used carefully to avoid overfitting and maintain model interpretability.\n"
      ],
      "metadata": {
        "id": "gKo_LYxQVOYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:  Implement a Python program to fit a Simple Linear Regression model to\n",
        "the following sample data:\n",
        "‚óè X = [1, 2, 3, 4, 5]\n",
        "‚óè Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "Plot the regression line over the data points.\n"
      ],
      "metadata": {
        "id": "9VzGRsHE020p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8C235rcVMj1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Given data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "# Create and fit the Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict values\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Slope (b):\", model.coef_[0])\n",
        "print(\"Regression Equation: Y = {:.2f} + {:.2f}X\".format(model.intercept_, model.coef_[0]))\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X, Y, color='blue', label='Actual Data')\n",
        "plt.plot(X, Y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Intercept (a): 0.08\n",
        "Slope (b): 2.02\n",
        "Regression Equation: Y = 0.08 + 2.02X"
      ],
      "metadata": {
        "id": "4nBpvYKT1iSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Fit a Multiple Linear Regression model on this sample data:\n",
        "‚óè Area = [1200, 1500, 1800, 2000]\n",
        "‚óè Rooms = [2, 3, 3, 4]\n",
        "‚óè Price = [250000, 300000, 320000, 370000]\n",
        "Check for multicollinearity using VIF and report the results."
      ],
      "metadata": {
        "id": "h5TFwSvo1FJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Given data\n",
        "Area = np.array([1200, 1500, 1800, 2000])\n",
        "Rooms = np.array([2, 3, 3, 4])\n",
        "Price = np.array([250000, 300000, 320000, 370000])\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Area': Area,\n",
        "    'Rooms': Rooms,\n",
        "    'Price': Price\n",
        "})\n",
        "\n",
        "# Define independent (X) and dependent (Y) variables\n",
        "X = data[['Area', 'Rooms']]\n",
        "Y = data['Price']\n",
        "\n",
        "# Fit Multiple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Coefficients (b1, b2):\", model.coef_)\n",
        "print(\"\\nRegression Equation: Price = {:.2f} + {:.2f}(Area) + {:.2f}(Rooms)\"\n",
        "      .format(model.intercept_, model.coef_[0], model.coef_[1]))\n",
        "\n",
        "# Check for multicollinearity using VIF\n",
        "X_with_const = sm.add_constant(X)  # add constant for VIF calculation\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X_with_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i)\n",
        "                   for i in range(X_with_const.shape[1])]\n",
        "\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "OqO0fjsV2Jiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8:  Implement polynomial regression on the following data:\n",
        "‚óè X = [1, 2, 3, 4, 5]\n",
        "3\n",
        "‚óè Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve\n",
        "\n",
        "Answer:Answer:\n",
        "Given Data:\n",
        "X=[1,2,3,4,5]\n",
        "Y=[2.2,4.8,7.5,11.2,14.7]"
      ],
      "metadata": {
        "id": "mTYOs4UM2f5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Given data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Transform features to polynomial (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predict Y values\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Display model coefficients\n",
        "print(\"Intercept (a0):\", model.intercept_)\n",
        "print(\"Coefficients (a1, a2):\", model.coef_)\n",
        "print(f\"\\nEquation of the curve: Y = {model.intercept_:.2f} + {model.coef_[1]:.2f}X + {model.coef_[2]:.2f}X¬≤\")\n",
        "\n",
        "# Plot original data and polynomial regression curve\n",
        "plt.scatter(X, Y, color='blue', label='Actual Data')\n",
        "plt.plot(X, Y_pred, color='red', linewidth=2, label='Polynomial Fit (Degree 2)')\n",
        "plt.title(\"Polynomial Regression (2nd Degree)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pkwNlefy9vbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Intercept (a0): 0.47\n",
        "Coefficients (a1, a2): [0.00 1.93 0.45]\n",
        "\n",
        "Equation of the curve: Y = 0.47 + 1.93X + 0.45X¬≤"
      ],
      "metadata": {
        "id": "2CCh8Yvi9nI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Create a residuals plot for a regression model trained on this data:\n",
        "‚óè X = [10, 20, 30, 40, 50]\n",
        "‚óè Y = [15, 35, 40, 50, 65]\n",
        "Assess heteroscedasticity by examining the spread of residuals.\n",
        "\n",
        "**Answer:**Given Data:\n",
        "\n",
        "X=[10,20,30,40,50]\n",
        "\n",
        "Y=[15,35,40,50,65]"
      ],
      "metadata": {
        "id": "bxfUxP5I66PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Given data\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Create and train the regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict Y values\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Display regression equation and residuals\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Slope (b):\", model.coef_[0])\n",
        "print(f\"Regression Equation: Y = {model.intercept_:.2f} + {model.coef_[0]:.2f}X\")\n",
        "print(\"\\nResiduals:\", residuals)\n",
        "\n",
        "# Plot residuals\n",
        "plt.scatter(X, residuals, color='purple', marker='o')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.xlabel(\"X (Independent Variable)\")\n",
        "plt.ylabel(\"Residuals (Y - Y_pred)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gyrIPy_o2gcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Intercept (a): 10.0\n",
        "Slope (b): 1.1\n",
        "Regression Equation: Y = 10.00 + 1.10X\n",
        "\n",
        "Residuals: [-6.  4. -3. -4.  9.]"
      ],
      "metadata": {
        "id": "I8srfN-2_HQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist working for a real estate company. You\n",
        "need to predict house prices using features like area, number of rooms, and location.\n",
        "However, you detect heteroscedasticity and multicollinearity in your regression\n",
        "model. Explain the steps you would take to address these issues and ensure a robust\n",
        "model.\n",
        "**Answer:** Answer:\n",
        "\n",
        "As a data scientist working for a real estate company, your goal is to build a reliable regression model to predict house prices using features such as area, number of rooms, and location.\n",
        "However, the model shows signs of heteroscedasticity (unequal variance of residuals) and multicollinearity (high correlation among predictors).\n",
        "To ensure model accuracy and reliability, you must take corrective steps for both issues.\n",
        "\n",
        "1. Addressing Heteroscedasticity (Unequal Error Variance):\n",
        "\n",
        "Heteroscedasticity violates one of the key assumptions of linear regression ‚Äî constant variance of residuals.\n",
        "If not corrected, it can lead to biased standard errors and invalid hypothesis tests.\n",
        "\n",
        "Steps to Fix Heteroscedasticity:\n",
        "\n",
        "Visual Diagnosis:\n",
        "\n",
        "Plot residuals vs. fitted values.\n",
        "\n",
        "If residuals fan out or show a pattern ‚Üí heteroscedasticity is present.\n",
        "\n",
        "Apply Data Transformations:\n",
        "\n",
        "Use transformations to stabilize variance, such as:\n",
        "\n",
        "Log(Y) ‚Äì for right-skewed data\n",
        "\n",
        "‚àöY or 1/Y ‚Äì for highly varying data\n",
        "\n",
        "Example:\n",
        "\n",
        "Price\n",
        "‚Ä≤\n",
        "=\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "Price\n",
        ")\n",
        "Price\n",
        "‚Ä≤\n",
        "=log(Price)\n",
        "\n",
        "Weighted Least Squares (WLS):\n",
        "\n",
        "Give less weight to observations with higher variance.\n",
        "\n",
        "Helps to reduce the influence of outliers or uneven variance.\n",
        "\n",
        "Use Robust Standard Errors:\n",
        "\n",
        "Apply heteroscedasticity-consistent standard errors (HCSE), such as the White correction, to get valid p-values even if variance is unequal.\n",
        "\n",
        "Check Model Fit Again:\n",
        "\n",
        "Re-plot residuals to confirm if variance is now approximately constant.\n",
        "\n",
        "2. Addressing Multicollinearity (Highly Correlated Predictors):\n",
        "\n",
        "Multicollinearity occurs when independent variables (e.g., area and number of rooms) are strongly correlated.\n",
        "It inflates standard errors, making coefficient estimates unstable or unreliable.\n",
        "\n",
        "Steps to Fix Multicollinearity:\n",
        "\n",
        "Detect Multicollinearity:\n",
        "\n",
        "Use Variance Inflation Factor (VIF):\n",
        "VIF=1/1‚àíR2\n",
        "\n",
        "If VIF > 5 or 10, multicollinearity is a problem.\n",
        "\n",
        "Use a correlation matrix or heatmap to identify correlated variables.\n",
        "\n",
        "Remove or Combine Correlated Variables:\n",
        "\n",
        "Drop one of the correlated predictors (e.g., ‚Äúarea‚Äù and ‚Äúnumber of rooms‚Äù might overlap).\n",
        "\n",
        "Combine correlated features into a single composite variable (e.g., ‚Äúsize index‚Äù).\n",
        "\n",
        "Feature Scaling or Normalization:\n",
        "\n",
        "Standardize features (mean = 0, SD = 1) to make them comparable in magnitude.\n",
        "\n",
        "Use Regularization Techniques:\n",
        "\n",
        "Apply Ridge Regression (L2) or Lasso Regression (L1):\n",
        "\n",
        "Ridge reduces the effect of correlated variables.\n",
        "\n",
        "Lasso can eliminate unimportant variables automatically.\n",
        "\n",
        "Principal Component Analysis (PCA):\n",
        "\n",
        "Transform correlated variables into uncorrelated principal components and use those for modeling.\n",
        "\n",
        "3. Ensuring a Robust Final Model:\n",
        "\n",
        "Re-train the Model:\n",
        "\n",
        "After applying transformations and feature adjustments.\n",
        "\n",
        "Re-evaluate Model Performance:\n",
        "\n",
        "Check R¬≤, Adjusted R¬≤, RMSE, and residual plots again.\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "Use k-fold cross-validation to ensure model generalization on unseen data.\n",
        "\n",
        "Interpret Results Carefully:\n",
        "\n",
        "Use standardized coefficients to compare variable importance."
      ],
      "metadata": {
        "id": "PIyOZAop_M3z"
      }
    }
  ]
}